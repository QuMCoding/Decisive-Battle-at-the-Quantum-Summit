{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NR1fbnMhN2t"
      },
      "source": [
        "# Code 12-1-2 如何在 Google Colaboratory 運行 Llama3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRN7YuBXhY7y"
      },
      "source": [
        "## Code 12-1-2-1 安裝套件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pT0Q3CdY7r9Z"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l01F6d-Jhi9n"
      },
      "source": [
        "## Code 12-1-2-2 引用套件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8cz5iHE8k8E1"
      },
      "outputs": [],
      "source": [
        "from llama_cpp import Llama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xgEAQQQhrpu"
      },
      "source": [
        "## Code 12-1-2-3 下載模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178,
          "referenced_widgets": [
            "8c28ad664d7e4a8687b875aecab028fa",
            "fa9d319f7be74b24ba7e8caf675c1473",
            "973c47910e214fc4b870983d955f077d",
            "887425a559d14fa8ab4801e477b925b6",
            "4ac5d5fef76a4b8a9daf1b70d67132b5",
            "01651308079c442497d4cb941f73776d",
            "d121e2d8599a430fb07645f6ee57dc25",
            "b2d1ef992b194bf2a5f4b0aa1b8c80a8",
            "d902d6af62e54af59eda3531a1b5c441",
            "c8d37f6fdcc74db6b7e093d75c628753",
            "d3304f5da1524981adadba009ea0ed18"
          ]
        },
        "id": "n6Oa96CGhBkF",
        "outputId": "56c9045b-aaf9-4f39-8c2e-3cd91f9e8d35"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c28ad664d7e4a8687b875aecab028fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Meta-Llama-3-8B-Instruct.Q4_K_M.gguf:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "llm = Llama.from_pretrained(\n",
        "    repo_id='QuantFactory/Meta-Llama-3-8B-Instruct-GGUF',\n",
        "    filename='Meta-Llama-3-8B-Instruct.Q4_K_M.gguf', # 使用 Q4_K_M 的量化版本\n",
        "    n_ctx=2048,\n",
        "    verbose=False # 不顯示輸出\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGBv7kyyh1xq"
      },
      "source": [
        "## Code 12-1-2-4 設定 prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "b4aDSpPghJaS"
      },
      "outputs": [],
      "source": [
        "# 創建歷史訊息\n",
        "messages = [\n",
        "    {\n",
        "        'role': 'system',\n",
        "        'content':'You are a professional physics master, you can answer the physics questions you know.' # 在聊天前先告訴模型他是個物理大師\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWqvr5ZIh8c9"
      },
      "source": [
        "## Code 12-1-2-5 聊天迴圈"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "bYYkr3xqhFG_",
        "outputId": "7b46bc14-6d73-4e7b-be80-aa7e5c56df34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[94mYou: \u001b[0msqrt((8.9-1.9)*3.2^2)=?\n",
            "\u001b[95mPhysics Master: \u001b[0mA nice math problem!\n",
            "\n",
            "Let's break it down step by step:\n",
            "\n",
            "1. Evaluate the expression inside the square root: `(8.9-1.9)*3.2^2`\n",
            "= `(7.0)*3.2^2` (since 8.9 - 1.9 = 7.0)\n",
            "= `(7.0)*(3.2)^2` (since 3.2^2 = 10.24)\n",
            "= `(7.0)*(10.24)`\n",
            "= 71.68\n",
            "\n",
            "2. Take the square root of the result: `sqrt(71.68)`\n",
            "≈ 8.51\n",
            "\n",
            "So, the final answer is: `sqrt((8.9-1.9)*3.2^2) ≈ 8.51`\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-0d6f2c2f351f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\033[94mYou: \\033[0m'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 使用者由此輸入\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0muser_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'role'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'user'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'content'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_message\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 將對話內容加入歷史訊息\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     response = llm.create_chat_completion(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    user_input = input('\\033[94mYou: \\033[0m') # 使用者由此輸入\n",
        "    user_message = {'role': 'user', 'content': user_input}\n",
        "    messages.append(user_message) # 將對話內容加入歷史訊息\n",
        "    response = llm.create_chat_completion(\n",
        "        messages=messages,\n",
        "        stream=True\n",
        "    )\n",
        "\n",
        "    # 以串流方式呈現文字輸出\n",
        "    for chunk in response:\n",
        "        delta = chunk['choices'][0]['delta']\n",
        "        if 'role' in delta:\n",
        "            messages.append({'role': delta['role'], 'content': ''})\n",
        "            print(\"\\033[95mPhysics Master: \\033[0m\", end='', flush=True)\n",
        "        elif 'content' in delta:\n",
        "            token = delta['content']\n",
        "            messages[-1]['content'] += token # 將對話內容加入歷史訊息\n",
        "            print(token, end=\"\", flush=True)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhAGOjVP7Es7"
      },
      "source": [
        "# Code 12-2-2 Wolfram|Alpha 與 Llama3 的結合"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47R-LlCU8MSm"
      },
      "source": [
        "## Code 12-2-2-1 設定 App ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jftzVYtp8YtP"
      },
      "outputs": [],
      "source": [
        "WOLFRAMALPHA_KEY = 'WH46VK-6Q5UT7PJ8R' # 將上一章節中取得的 App ID 複製至這裡"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr0eKuYU7ohl"
      },
      "source": [
        "## Code 12-2-2-2 引用套件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Smmg6hWz8usA"
      },
      "outputs": [],
      "source": [
        "import urllib.parse\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esZvUTOx83HF"
      },
      "source": [
        "## Code 12-2-2-3 編寫 Wolfram|Alpha 呼叫函式"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "e2PV5hsu7n_h"
      },
      "outputs": [],
      "source": [
        "def ask_wolframalpha(query):\n",
        "    query = urllib.parse.quote_plus('calculate ' + query) # 將問題進行 URL 編碼，並且將空格替換為加號。\n",
        "    query_url = ''.join([\n",
        "        f\"http://api.wolframalpha.com/v2/query?\", # Wolfram|Alpha Full Results API 網址\n",
        "        f\"appid={WOLFRAMALPHA_KEY}\", # 填入 App ID\n",
        "        f\"&input={query}\", # 填入問題\n",
        "        f\"&includepodid=Result\", # 只回傳結果\n",
        "        f\"&output=json\" # 輸出 json 格式\n",
        "    ])\n",
        "    r = requests.get(query_url) # 發出 GET 請求\n",
        "    if r.status_code == 200: # 如果請求成功則回傳答案\n",
        "        try:\n",
        "            return r.json()[\"queryresult\"][\"pods\"][0][\"subpods\"][0][\"plaintext\"]\n",
        "        except: ...\n",
        "    return '\\n\\nSorry. There are some problems with wolframalpha. I will not use wolframalpha.' # 如果請求失敗則回傳提示"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1MwURbLKUCX"
      },
      "source": [
        "## Code 12-2-2-4 歷史訊息轉提示"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "DfOxzyNjgrtn"
      },
      "outputs": [],
      "source": [
        "def prompt_from_messages(messages):\n",
        "    prompt = ''\n",
        "    for message in messages:\n",
        "        prompt += f\"<|start_header_id|>{message['role']}<|end_header_id|>\\n\\n\"\n",
        "        prompt += f\"{message['content']}<|eot_id|>\"\n",
        "    prompt = prompt[:-10] # 去除最後的 <|eot_id|>\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3sL7MDy4apm"
      },
      "source": [
        "## Code 12-2-2-5 重新設定 prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "CLqSUqPe4jdy"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\n",
        "        'role': 'system',\n",
        "        # 加入新的提示，告訴模型遇到不會的計算要輸出 <|wolframalpha|> 和問題\n",
        "        'content':'You are a professional physics master, you can answer the physics questions you know.' + ' You can use wolframalpha to help you calculate. If you are not sure about the answer, especially problems that require calculation or internet, you must use WolframAlpha. To use WolframAlpha, please ONLY output \"<|wolframalpha|>\" in the message, and then ONLY output the calculaion formula you want to use WolframAlpha to calculate and <|wolframalphaend|>. For example, \"<|wolframalpha|>x^2+2x+1=0<|wolframalphaend|>\" or \"<|wolframalpha|>0.6-0.25*2<|wolframalphaend|>\", a <|wolframalpha|> tag can ONLY use in a calculation formula, other information is not necessary. <|wolframalpha|x^2+2x+1=0|> is a wrong example. After system get the answer from WolframAlpha, you can continue to answer the question. If you have new question, please output \"<|wolframalpha|>your question here<|wolframalphaend|>\" again.'\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTZ_BQwX-UMK"
      },
      "source": [
        "## Code 12-2-2-6 在聊天迴圈中增加 Wolfram|Alpha 使用判斷機制"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "MT3AIC4e-TpW",
        "outputId": "c4eccd54-a4e2-4c44-ee5c-dd3f22980838"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[94mYou: \u001b[0m0.2*sqrt(8.1-2.4*2.21)=?\n",
            "\u001b[95mPhysics Master: \u001b[0mI'll use WolframAlpha to calculate the exact value:\n",
            "\n",
            "\n",
            "0.2*sqrt(8.1-2.4*2.21)\n",
            "\n",
            "According to WolframAlpha the answer is 0.334425...\n",
            "\u001b[94mYou: \u001b[0m"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-c9dbe98d58e3>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\033[94mYou: \\033[0m'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 使用者由此輸入\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0muser_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'role'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'user'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'content'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_message\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 將對話內容加入歷史訊息\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "use_wolframalpha = False # 是否使用 WolframAlpha\n",
        "\n",
        "while True:\n",
        "    print()\n",
        "    if use_wolframalpha: # 如果使用 WolframAlpha\n",
        "        response = llm.create_completion( # 模型依照 WolframAlpha 的回答繼續接話\n",
        "            prompt=prompt_from_messages(messages),\n",
        "            stream=True\n",
        "        )\n",
        "        use_wolframalpha = False\n",
        "        for token in wolframalpha_token:\n",
        "            print(token, end=\"\", flush=True)\n",
        "    else:\n",
        "        print('\\033[94mYou: \\033[0m', end='')\n",
        "        user_input = input() # 使用者由此輸入\n",
        "        user_message = {'role': 'user', 'content': user_input}\n",
        "        messages.append(user_message) # 將對話內容加入歷史訊息\n",
        "        response = llm.create_chat_completion( # 模型依照歷史訊息聊天\n",
        "            messages=messages,\n",
        "            stream=True\n",
        "        )\n",
        "        messages.append({'role': 'assistant', 'content': ''})\n",
        "        print(\"\\033[95mPhysics Master: \\033[0m\", end='', flush=True)\n",
        "\n",
        "    temp_token = ''\n",
        "    question = ''\n",
        "\n",
        "    # 以串流方式呈現文字輸出\n",
        "    for chunk in response:\n",
        "        if 'delta' in chunk['choices'][0]:\n",
        "            delta = chunk['choices'][0]['delta'] # 聊天模式的結構\n",
        "        else:\n",
        "            delta = {'content': chunk['choices'][0]['text']} # 接話模式的結構\n",
        "        if 'content' in delta:\n",
        "            token = delta['content']\n",
        "            temp_token += token\n",
        "            if '<|wolframalpha|>' in temp_token: # 如果輸出包含 <|wolframalpha|> 就使用 WolframAlpha\n",
        "                use_wolframalpha = True\n",
        "                question = temp_token.split('<|wolframalpha|>')[-1]\n",
        "                temp_token = ''\n",
        "            if use_wolframalpha:\n",
        "                if temp_token: question += token # 將輸出加入問題\n",
        "                if '\\n\\n' in question or '<|wolframalphaend|>' in question: # 如果輸出包含 <|wolframalphaend|> 就計算問題\n",
        "                    question = question.replace('\\n', '').replace('<|wolframalphaend|>', '')\n",
        "                    answer = ask_wolframalpha(question)\n",
        "                    wolframalpha_token = question + '\\n\\nAccording to WolframAlpha the answer is ' + answer\n",
        "                    messages[-1]['content'] += wolframalpha_token\n",
        "                    break\n",
        "                continue\n",
        "            if temp_token != '<|wolframalpha|>'[:len(temp_token)] and '<|wolframalpha|>' not in temp_token: # 如果輸出不包含 <|wolframalpha|> 就輸出\n",
        "                messages[-1]['content'] += temp_token # 將對話內容加入歷史訊息\n",
        "                print(temp_token, end=\"\", flush=True)\n",
        "                temp_token = ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR7sDiZ-ox7q"
      },
      "source": [
        "# Code 12-3 利用 Llama 學習物理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hyL9CT-oNRi"
      },
      "source": [
        "## Code 12-3-0-1 載入微調的模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxDGIeZSoPmT"
      },
      "outputs": [],
      "source": [
        "llm = Llama.from_pretrained(\n",
        "    repo_id='gallen881/Meta-Llama-3-8B-Instruct-GGUF', # 使用自己微調的模型\n",
        "    filename='Meta-Llama-3-8B-Instruct.Q4_K_M.gguf', # 使用自己的模型名稱\n",
        "    n_ctx=2048,\n",
        "    verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teqmHP9tovhm"
      },
      "source": [
        "## Code 12-3-0-2 依照先前的方法推論"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8V2iusHpjau"
      },
      "source": [
        "以下程式與 Code 12-2-2-5、Code 12-2-2-6 相同，使用一樣的聊天方法，Wolfram|Alpha一樣適用。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwFijpN4pgCK"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\n",
        "        'role': 'system',\n",
        "        'content':'You are a professional physics master, you can answer the physics questions you know.' + ' You can use wolframalpha to help you calculate. If you are not sure about the answer, especially problems that require calculation or internet, you must use WolframAlpha. To use WolframAlpha, please ONLY output \"<|wolframalpha|>\" in the message, and then ONLY output the calculaion formula you want to use WolframAlpha to calculate and <|wolframalphaend|>. For example, \"<|wolframalpha|>x^2+2x+1=0<|wolframalphaend|>\" or \"<|wolframalpha|>0.6-0.25*2<|wolframalphaend|>\", a <|wolframalpha|> tag can ONLY use in a calculation formula, other information is not necessary. <|wolframalpha|x^2+2x+1=0|> is a wrong example. After system get the answer from WolframAlpha, you can continue to answer the question. If you have new question, please output \"<|wolframalpha|>your question here<|wolframalphaend|>\" again.'\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjfvvGPBpgCL"
      },
      "outputs": [],
      "source": [
        "use_wolframalpha = False # 是否使用 WolframAlpha\n",
        "\n",
        "while True:\n",
        "    print()\n",
        "    if use_wolframalpha: # 如果使用 WolframAlpha\n",
        "        response = llm.create_completion( # 模型依照 WolframAlpha 的回答繼續接話\n",
        "            prompt=prompt_from_messages(messages),\n",
        "            stream=True\n",
        "        )\n",
        "        use_wolframalpha = False\n",
        "        for token in wolframalpha_token:\n",
        "            print(token, end=\"\", flush=True)\n",
        "    else:\n",
        "        print('\\033[94mYou: \\033[0m', end='')\n",
        "        user_input = input() # 使用者由此輸入\n",
        "        user_message = {'role': 'user', 'content': user_input}\n",
        "        messages.append(user_message) # 將對話內容加入歷史訊息\n",
        "        response = llm.create_chat_completion( # 模型依照歷史訊息聊天\n",
        "            messages=messages,\n",
        "            stream=True\n",
        "        )\n",
        "        messages.append({'role': 'assistant', 'content': ''})\n",
        "        print(\"\\033[95mPhysics Master: \\033[0m\", end='', flush=True)\n",
        "\n",
        "    temp_token = ''\n",
        "    question = ''\n",
        "\n",
        "    # 以串流方式呈現文字輸出\n",
        "    for chunk in response:\n",
        "        if 'delta' in chunk['choices'][0]:\n",
        "            delta = chunk['choices'][0]['delta'] # 聊天模式的結構\n",
        "        else:\n",
        "            delta = {'content': chunk['choices'][0]['text']} # 接話模式的結構\n",
        "        if 'content' in delta:\n",
        "            token = delta['content']\n",
        "            temp_token += token\n",
        "            if '<|wolframalpha|>' in temp_token: # 如果輸出包含 <|wolframalpha|> 就使用 WolframAlpha\n",
        "                use_wolframalpha = True\n",
        "                question = temp_token.split('<|wolframalpha|>')[-1]\n",
        "                temp_token = ''\n",
        "            if use_wolframalpha:\n",
        "                if temp_token: question += token # 將輸出加入問題\n",
        "                if '\\n\\n' in question or '<|wolframalphaend|>' in question: # 如果輸出包含 <|wolframalphaend|> 就計算問題\n",
        "                    question = question.replace('\\n', '').replace('<|wolframalphaend|>', '')\n",
        "                    answer = ask_wolframalpha(question)\n",
        "                    wolframalpha_token = question + '\\n\\nAccording to WolframAlpha the answer is ' + answer\n",
        "                    messages[-1]['content'] += wolframalpha_token\n",
        "                    break\n",
        "                continue\n",
        "            if temp_token != '<|wolframalpha|>'[:len(temp_token)] and '<|wolframalpha|>' not in temp_token: # 如果輸出不包含 <|wolframalpha|> 就輸出\n",
        "                messages[-1]['content'] += temp_token # 將對話內容加入歷史訊息\n",
        "                print(temp_token, end=\"\", flush=True)\n",
        "                temp_token = ''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01651308079c442497d4cb941f73776d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ac5d5fef76a4b8a9daf1b70d67132b5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "887425a559d14fa8ab4801e477b925b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8d37f6fdcc74db6b7e093d75c628753",
            "placeholder": "​",
            "style": "IPY_MODEL_d3304f5da1524981adadba009ea0ed18",
            "value": " 4.92G/4.92G [01:13&lt;00:00, 127MB/s]"
          }
        },
        "8c28ad664d7e4a8687b875aecab028fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa9d319f7be74b24ba7e8caf675c1473",
              "IPY_MODEL_973c47910e214fc4b870983d955f077d",
              "IPY_MODEL_887425a559d14fa8ab4801e477b925b6"
            ],
            "layout": "IPY_MODEL_4ac5d5fef76a4b8a9daf1b70d67132b5"
          }
        },
        "973c47910e214fc4b870983d955f077d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2d1ef992b194bf2a5f4b0aa1b8c80a8",
            "max": 4920733952,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d902d6af62e54af59eda3531a1b5c441",
            "value": 4920733952
          }
        },
        "b2d1ef992b194bf2a5f4b0aa1b8c80a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8d37f6fdcc74db6b7e093d75c628753": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d121e2d8599a430fb07645f6ee57dc25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3304f5da1524981adadba009ea0ed18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d902d6af62e54af59eda3531a1b5c441": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fa9d319f7be74b24ba7e8caf675c1473": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01651308079c442497d4cb941f73776d",
            "placeholder": "​",
            "style": "IPY_MODEL_d121e2d8599a430fb07645f6ee57dc25",
            "value": "Meta-Llama-3-8B-Instruct.Q4_K_M.gguf: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
